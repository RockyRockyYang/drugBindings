{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0f9520ba11fdc1099356e05d27a34ed870b487e4078e45fe1cddd0198c8b5354c",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## AM 216 Final Project\n",
    "Team Member: Lihong Zhang, Litao Yan, Ruoxi Yang\n",
    "\n",
    "Deep learning bears promise for drug discovery, and recently we have seen many prospective application in the field of drug-target interaction. However, the underlying mathematical models often remain elusive to interpretation, since ofen time deep pearning can only be treated as black box and hard to interpret. \n",
    "For this final project, inspired by Drug-target interaction miniproject from this course, we want to analyze explainabliity of different data representations. \n",
    "\n",
    "We explore the representations and their visualization from different perspective. Due to the time limit, we write the final project in 2 parts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1 Graph Neural Networks\n",
    "Inspired by section 10, we intend to apply Graph Neural Networks(GNN) in solving drug-protein binding affinity problem and see whether GNN would improve the prediction results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rdkit\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential, Model, Input\n",
    "from keras.layers import Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from spektral.layers import GATConv, ChebConv\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import networkx as nx\n",
    "\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "import deepchem as dc"
   ]
  },
  {
   "source": [
    "Prepare train data and test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_KIBA_PATH = '../data/mini_project_data/data/kiba/'\n",
    "LOCAL_DAVIS_PATH = '../data/GraphDTA_davis/'\n",
    "G_PATH = './drive/MyDrive/Colab Notebooks/Drug Binding'\n",
    "max_seq_len = 1000\n",
    "# for converting protein sequence to categorical format/numerical format\n",
    "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\n",
    "seq_dict = {v:i for i,v in enumerate(seq_voc)}\n",
    "seq_dict_len = len(seq_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! 13 2x2 matrices, nodes + adj together\n",
    "\n",
    "def adj2matr(raw_a):\n",
    "    len_a = len(raw_a)\n",
    "    matr = np.zeros((len_a, len_a))\n",
    "    for i in range(len_a):\n",
    "        for j in raw_a[i]:\n",
    "            matr[i, j] = 1\n",
    "    return matr\n",
    "\n",
    "# convmol to matrices\n",
    "def conv2matr(convmol):\n",
    "    nr, nc = max([convmol[i].get_atom_features().shape[0] for i in range(len(convmol))]), convmol[0].get_atom_features().shape[1]\n",
    "    amax = max([len(convmol[i].get_adjacency_list()) for i in range(len(convmol))])\n",
    "    ar, ac = amax, amax\n",
    "    nodes = []\n",
    "    adj = []\n",
    "    for i in range(len(convmol)):\n",
    "        temp_n = np.zeros((nr, nc))\n",
    "        n = convmol[i].get_atom_features()\n",
    "        temp_n[:n.shape[0],:n.shape[1]] = n\n",
    "        # nodes.append(temp_n)\n",
    "        nodes.append(temp_n.reshape((1, temp_n.shape[0], temp_n.shape[1])))\n",
    "        temp_a = np.zeros((ar, ac))\n",
    "        a = adj2matr(convmol[i].get_adjacency_list()) \n",
    "        temp_a[:a.shape[0],:a.shape[1]] = a\n",
    "        # adj.append(temp_a)\n",
    "        adj.append(temp_a.reshape((1, temp_a.shape[0], temp_a.shape[1])))\n",
    "    return [np.concatenate(nodes, axis = 0), np.concatenate(adj, axis = 0)]\n",
    "    # return [nodes, adj]\n",
    "\n",
    "\n",
    "\n",
    "# The codes below in this cell are from section 10 of AM216 and mini project\n",
    "def seq_to_cat(prot):  # prot: protein\n",
    "    x = np.zeros(max_seq_len)\n",
    "    for i, ch in enumerate(prot[:max_seq_len]): \n",
    "        x[i] = seq_dict[ch]\n",
    "    return x  \n",
    "\n",
    "def read_data(data_path):\n",
    "    drugs_ = json.load(open(data_path + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
    "    \n",
    "    smiles = np.array([Chem.MolToSmiles(Chem.MolFromSmiles(d),isomericSmiles=True) for d in drugs_.values()])\n",
    "    featurizer=dc.feat.ConvMolFeaturizer(per_atom_fragmentation=False)\n",
    "    convmol = featurizer.featurize(smiles)\n",
    "\n",
    "    # inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "    # for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "      # inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "      \n",
    "    # drugs = conv2matr(convmol)\n",
    "\n",
    "    proteins_ = json.load(open(data_path + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
    "    proteins = np.array(list(proteins_.values()))\n",
    "    affinity = np.array(pickle.load(open(data_path + \"Y\",\"rb\"), encoding='latin1'))\n",
    "    train_fold = json.load(open(data_path + \"folds/train_fold_setting1.txt\"))\n",
    "    train_fold = [ee for e in train_fold for ee in e ]    \n",
    "    test_fold = json.load(open(data_path + \"folds/test_fold_setting1.txt\"))\n",
    "\n",
    "    # Prepare train/test data with fold indices\n",
    "    rows, cols = np.where(np.isnan(affinity)==False) \n",
    "    convmol_tr = convmol[rows[train_fold]]    # (98545,)\n",
    "    smiles_tr = smiles[rows[train_fold]] \n",
    "    proteins_tr = np.array([seq_to_cat(p) for p in proteins[cols[train_fold]]])   # (98545, 1000)\n",
    "    affinity_tr = affinity[rows[train_fold], cols[train_fold]]  # (98545,)\n",
    "\n",
    "    convmol_ts = convmol[rows[test_fold]] # (19709,)\n",
    "    smiles_ts = smiles[rows[test_fold]] # (19709,)\n",
    "    proteins_ts = np.array([seq_to_cat(p) for p in proteins[cols[test_fold]]]) # (19709, 1000)\n",
    "    affinity_ts = affinity[rows[test_fold], cols[test_fold]]    # (19709,)\n",
    "    '''\n",
    "    print('Example of drug:{}'.format(drugs_tr[0]))\n",
    "    print('Example of protein:{} ...'.format(proteins_tr[0][:10]))\n",
    "    print('Example of affinity score:{}'.format(affinity_tr[0]))\n",
    "    '''\n",
    "    return convmol_tr, smiles_tr, proteins_tr, affinity_tr, convmol_ts, smiles_ts, proteins_ts, affinity_ts\n",
    "\n",
    "def smiles_graph(path):\n",
    "    drugs_ = json.load(open(path + 'ligands_can.txt'), object_pairs_hook=OrderedDict)\n",
    "    # print('\\nOriginal molecule:')\n",
    "    mols = MolFromSmiles(smiles[0])\n",
    "    # Draw.MolToImage(mols)\n",
    "    featurizer=dc.feat.ConvMolFeaturizer(per_atom_fragmentation=False)\n",
    "    graph_data = featurizer.featurize(smiles)\n",
    "    return smiles, graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr, nc = max([convmol_tr_kiba[i].get_atom_features().shape[0] for i in range(len(convmol_tr_kiba))]), convmol_tr_kiba[0].get_atom_features().shape[1]"
   ]
  },
  {
   "source": [
    "Convert SMILES to Graphs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convmol_tr_kiba, smiles_tr_kiba, proteins_tr_kiba, affinity_tr_kiba, convmol_ts_kiba, smiles_ts_kiba, proteins_ts_kiba, affinity_ts_kiba = read_data(LOCAL_KIBA_PATH)\n",
    "convmol_tr_davis, smiles_tr_davis, proteins_tr_davis, affinity_tr_davis, convmol_ts_davis, smiles_ts_davis, proteins_ts_davis, affinity_ts_davis = read_data(LOCAL_DAVIS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25046"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the nodes and adjacency matrix of drugs\n",
    "# nodes_tr_kiba, adj_tr_kiba = conv2matr(convmol_tr_kiba)\n",
    "# nodes_ts_kiba, adj_ts_kiba = conv2matr(convmol_ts_kiba)\n",
    "\n",
    "nodes_tr_davis, adj_tr_davis = conv2matr(convmol_tr_davis)\n",
    "nodes_ts_davis, adj_ts_davis = conv2matr(convmol_ts_davis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25046, 46, 75) (25046, 46, 46)\n"
     ]
    }
   ],
   "source": [
    "# print(nodes_ts_kiba.shape, nodes_ts_kiba[0].shape, adj_ts_kiba.shape)\n",
    "print(nodes_tr_davis.shape, adj_tr_davis.shape)"
   ]
  },
  {
   "source": [
    "Prepare data for CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_size, drug_size = drugs_ecfp_tr.shape[0], drugs_ecfp_tr.shape[1]\n",
    "num_ts_davis = proteins_ts_davis.shape[0]\n",
    "\n",
    "# num_train, num_drugs = drugs_ecfp_tr.shape\n",
    "num_tr_davis, num_prot_davis = proteins_tr_davis.shape\n",
    "# drugs_tr_reshape = drugs_ecfp_tr.reshape((num_train, num_drugs, 1))\n",
    "proteins_tr_davis_reshape = proteins_tr_davis.reshape((num_tr_davis, num_prot_davis, 1))\n",
    "\n",
    "# Testing data\n",
    "# drug_ts_reshape = drugs_ecfp_ts.reshape((drugs_ecfp_ts.shape[0], drugs_ecfp_ts.shape[1], 1))\n",
    "# proteins_ts_reshape = proteins_ts.reshape((proteins_ts.shape[0], proteins_ts.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25046,) (25046, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(affinity_tr_davis.shape, proteins_tr_davis_reshape.shape)"
   ]
  },
  {
   "source": [
    "Prepare data for GNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dc.data.NumpyDataset([convmol_tr_davis, proteins_tr_davis_reshape], affinity_tr_davis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_18 (InputLayer)           [(None, 46, 75)]     0                                            \n__________________________________________________________________________________________________\ninput_19 (InputLayer)           [(None, 46, 46)]     0                                            \n__________________________________________________________________________________________________\ngraph_input (GATConv)           (None, 46, 64)       4992        input_18[0][0]                   \n                                                                 input_19[0][0]                   \n__________________________________________________________________________________________________\ndropout_15 (Dropout)            (None, 46, 64)       0           graph_input[0][0]                \n__________________________________________________________________________________________________\ncheb_conv_6 (ChebConv)          (None, 46, 32)       2080        dropout_15[0][0]                 \n                                                                 input_19[0][0]                   \n__________________________________________________________________________________________________\ninput_17 (InputLayer)           [(None, 1000, 1)]    0                                            \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 46, 32)       0           cheb_conv_6[0][0]                \n__________________________________________________________________________________________________\nconv1d_5 (Conv1D)               (None, 998, 16)      64          input_17[0][0]                   \n__________________________________________________________________________________________________\ngat_conv_3 (GATConv)            (None, 46, 64)       2240        dropout_16[0][0]                 \n                                                                 input_19[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling1d_5 (MaxPooling1D)  (None, 332, 16)      0           conv1d_5[0][0]                   \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 46, 64)       0           gat_conv_3[0][0]                 \n__________________________________________________________________________________________________\nflatten_8 (Flatten)             (None, 5312)         0           max_pooling1d_5[0][0]            \n__________________________________________________________________________________________________\ncheb_conv_7 (ChebConv)          (None, 46, 32)       2080        dropout_17[0][0]                 \n                                                                 input_19[0][0]                   \n__________________________________________________________________________________________________\ndropout_14 (Dropout)            (None, 5312)         0           flatten_8[0][0]                  \n__________________________________________________________________________________________________\nflatten_9 (Flatten)             (None, 1472)         0           cheb_conv_7[0][0]                \n__________________________________________________________________________________________________\ndense_13 (Dense)                (None, 16)           85008       dropout_14[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 1488)         0           flatten_9[0][0]                  \n                                                                 dense_13[0][0]                   \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 512)          762368      concatenate_3[0][0]              \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 256)          131328      dense_14[0][0]                   \n__________________________________________________________________________________________________\ndense_16 (Dense)                (None, 1)            257         dense_15[0][0]                   \n==================================================================================================\nTotal params: 990,417\nTrainable params: 990,417\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/62232435/error-in-multi-input-model-for-graph-neural-network-in-tf-keras\n",
    "\n",
    "# CNN for protein\n",
    "Feat_input = Input(shape=(num_prot_davis,1))\n",
    "Feat_layer = Conv1D(16, 3, activation='relu', input_shape=(num_prot_davis, 1))(Feat_input)\n",
    "Feat_layer = MaxPooling1D(3)(Feat_layer)\n",
    "Feat_layer = Flatten()(Feat_layer)\n",
    "Feat_layer = Dropout(0.1)(Feat_layer)\n",
    "Feat_layer = Dense(16, activation = 'linear')(Feat_layer)\n",
    "\n",
    "xs1, xs2 = nodes_ts_davis.shape[1], nodes_ts_davis.shape[2]\n",
    "as1, as2 = adj_tr_davis.shape[1], adj_tr_davis.shape[2]\n",
    "X_in = Input(shape=(xs1, xs2))\n",
    "A_in = Input(shape = (as1, as2))\n",
    "\n",
    "# GNN for \n",
    "graph_conv = GATConv(64, activation='relu',name='graph_input')([X_in, A_in])\n",
    "graph_conv = Dropout(0.5)(graph_conv)\n",
    "\n",
    "graph_conv = ChebConv(32, activation='relu')([graph_conv,A_in])\n",
    "\n",
    "graph_conv = Dropout(0.5)(graph_conv)\n",
    "\n",
    "graph_conv = GATConv(64, activation='relu')([graph_conv,A_in])\n",
    "graph_conv = Dropout(0.5)(graph_conv)\n",
    "graph_conv = ChebConv(32, activation='relu')([graph_conv, A_in])\n",
    "\n",
    "graph_conv = Flatten()(graph_conv)\n",
    "\n",
    "dta_concat = concatenate([graph_conv, Feat_layer])\n",
    "\n",
    "dta_concat = Dense(512, activation='relu')(dta_concat)\n",
    "dta_concat = Dense(256, activation='relu')(dta_concat)\n",
    "dta_concat = Dense(1, activation='softmax')(dta_concat)\n",
    "\n",
    "model = Model(inputs={'graph_input':[X_in, A_in], 'cnn_input':Feat_input}, outputs=dta_concat)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5455 - acc: 0.0000e+00\n",
      "Epoch 65/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 72060014.5455 - acc: 0.0000e+00\n",
      "Epoch 66/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 65278169.0909 - acc: 0.0000e+00\n",
      "Epoch 67/250\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 70486602.1818 - acc: 0.0000e+00\n",
      "Epoch 68/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 68107737.8182 - acc: 0.0000e+00\n",
      "Epoch 69/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 67558012.0000 - acc: 0.0000e+00\n",
      "Epoch 70/250\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 70086497.4545 - acc: 0.0000e+00\n",
      "Epoch 71/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 69853175.2727 - acc: 0.0000e+00\n",
      "Epoch 72/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 74343170.9091 - acc: 0.0000e+00\n",
      "Epoch 73/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 72277384.0000 - acc: 0.0000e+00\n",
      "Epoch 74/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 71710511.2727 - acc: 0.0000e+00\n",
      "Epoch 75/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 71886789.0909 - acc: 0.0000e+00\n",
      "Epoch 76/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 65127822.9091 - acc: 0.0024\n",
      "Epoch 77/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 70968126.5455 - acc: 0.0000e+00\n",
      "Epoch 78/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 69342517.8182 - acc: 0.0000e+00\n",
      "Epoch 79/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 72203436.7273 - acc: 0.0000e+00\n",
      "Epoch 80/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 70165985.0909 - acc: 0.0000e+00\n",
      "Epoch 81/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 69668537.4545 - acc: 0.0000e+00\n",
      "Epoch 82/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 79662519.2727 - acc: 0.0000e+00\n",
      "Epoch 83/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 75974928.7273 - acc: 0.0000e+00\n",
      "Epoch 84/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 69830981.4545 - acc: 0.0000e+00\n",
      "Epoch 85/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 67114809.0909 - acc: 0.0039\n",
      "Epoch 86/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 72074221.0909 - acc: 0.0000e+00\n",
      "Epoch 87/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 68006180.0000 - acc: 0.0000e+00\n",
      "Epoch 88/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 73959397.8182 - acc: 0.0000e+00\n",
      "Epoch 89/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 72128283.6364 - acc: 0.0000e+00\n",
      "Epoch 90/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 71319332.3636 - acc: 0.0000e+00\n",
      "Epoch 91/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 64907918.9091 - acc: 0.0000e+00\n",
      "Epoch 92/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 70766948.3636 - acc: 0.0098\n",
      "Epoch 93/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 72606984.7273 - acc: 0.0000e+00\n",
      "Epoch 94/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 72621384.0000 - acc: 0.0000e+00\n",
      "Epoch 95/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 67662957.0909 - acc: 0.0000e+00\n",
      "Epoch 96/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 70583391.2727 - acc: 0.0000e+00\n",
      "Epoch 97/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 69066661.0909 - acc: 0.0000e+00\n",
      "Epoch 98/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 65028323.6364 - acc: 6.4935e-04\n",
      "Epoch 99/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 72506575.2727 - acc: 0.0000e+00\n",
      "Epoch 100/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 79896322.9091 - acc: 0.0000e+00\n",
      "Epoch 101/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 68904085.0909 - acc: 0.0000e+00\n",
      "Epoch 102/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 74919994.9091 - acc: 0.0000e+00\n",
      "Epoch 103/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 73612135.2727 - acc: 0.0000e+00\n",
      "Epoch 104/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 73855300.3636 - acc: 0.0000e+00\n",
      "Epoch 105/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 72662560.0000 - acc: 6.4935e-04\n",
      "Epoch 106/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 75729265.4545 - acc: 0.0000e+00\n",
      "Epoch 107/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 68345913.4545 - acc: 0.0050\n",
      "Epoch 108/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 71382713.8182 - acc: 0.0000e+00\n",
      "Epoch 109/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 70755686.5455 - acc: 0.0000e+00\n",
      "Epoch 110/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 73628543.2727 - acc: 0.0000e+00\n",
      "Epoch 111/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 70854954.1818 - acc: 0.0000e+00\n",
      "Epoch 112/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 74093688.7273 - acc: 0.0000e+00\n",
      "Epoch 113/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 68897924.0000 - acc: 0.0000e+00\n",
      "Epoch 114/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 68668858.5455 - acc: 0.0000e+00\n",
      "Epoch 115/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 70993217.4545 - acc: 0.0000e+00\n",
      "Epoch 116/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 74552585.4545 - acc: 0.0000e+00\n",
      "Epoch 117/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 73862848.0000 - acc: 0.0000e+00\n",
      "Epoch 118/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 68276089.8182 - acc: 0.0000e+00\n",
      "Epoch 119/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 74677226.9091 - acc: 0.0000e+00\n",
      "Epoch 120/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 77487300.3636 - acc: 0.0014\n",
      "Epoch 121/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 69454004.3636 - acc: 0.0000e+00\n",
      "Epoch 122/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 71801456.7273 - acc: 0.0000e+00\n",
      "Epoch 123/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 70940654.1818 - acc: 0.0000e+00\n",
      "Epoch 124/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 70964499.6364 - acc: 0.0000e+00\n",
      "Epoch 125/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 72242634.9091 - acc: 0.0000e+00\n",
      "Epoch 126/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 67251805.0909 - acc: 0.0000e+00\n",
      "Epoch 127/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 76796026.1818 - acc: 0.0039\n",
      "Epoch 128/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 78332106.9091 - acc: 0.0000e+00\n",
      "Epoch 129/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 74701414.5455 - acc: 0.0000e+00\n",
      "Epoch 130/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 65980337.0909 - acc: 0.0000e+00\n",
      "Epoch 131/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 69480182.5455 - acc: 0.0000e+00\n",
      "Epoch 132/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 69376040.3636 - acc: 0.0000e+00\n",
      "Epoch 133/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 74430544.3636 - acc: 0.0000e+00\n",
      "Epoch 134/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 75157841.4545 - acc: 0.0039\n",
      "Epoch 135/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 69548470.5455 - acc: 0.0010\n",
      "Epoch 136/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 72920923.6364 - acc: 0.0000e+00\n",
      "Epoch 137/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 69581725.8182 - acc: 0.0000e+00\n",
      "Epoch 138/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 75442645.0909 - acc: 0.0000e+00\n",
      "Epoch 139/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 78373828.3636 - acc: 0.0000e+00\n",
      "Epoch 140/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 70582698.9091 - acc: 0.0000e+00\n",
      "Epoch 141/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 72904392.7273 - acc: 0.0014\n",
      "Epoch 142/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 73012718.5455 - acc: 0.0000e+00\n",
      "Epoch 143/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 73760598.5455 - acc: 0.0000e+00\n",
      "Epoch 144/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 78639483.6364 - acc: 0.0000e+00\n",
      "Epoch 145/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 73683657.4545 - acc: 0.0000e+00\n",
      "Epoch 146/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 73104143.2727 - acc: 0.0000e+00\n",
      "Epoch 147/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 71190696.7273 - acc: 0.0000e+00\n",
      "Epoch 148/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 67892170.1818 - acc: 0.0098\n",
      "Epoch 149/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 71348013.4545 - acc: 0.0000e+00\n",
      "Epoch 150/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 73328488.7273 - acc: 0.0000e+00\n",
      "Epoch 151/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 69029564.7273 - acc: 0.0000e+00\n",
      "Epoch 152/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 67665072.3636 - acc: 0.0000e+00\n",
      "Epoch 153/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 66777652.3636 - acc: 0.0000e+00\n",
      "Epoch 154/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 74402968.0000 - acc: 0.0000e+00\n",
      "Epoch 155/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 70340116.7273 - acc: 0.0000e+00\n",
      "Epoch 156/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 73958857.4545 - acc: 0.0000e+00\n",
      "Epoch 157/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 69180623.6364 - acc: 0.0000e+00\n",
      "Epoch 158/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 78782222.5455 - acc: 0.0000e+00\n",
      "Epoch 159/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 72601154.1818 - acc: 0.0000e+00\n",
      "Epoch 160/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 73042182.5455 - acc: 0.0000e+00\n",
      "Epoch 161/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 70107639.6364 - acc: 0.0000e+00\n",
      "Epoch 162/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 75684692.3636 - acc: 0.0000e+00\n",
      "Epoch 163/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 69057928.3636 - acc: 0.0000e+00\n",
      "Epoch 164/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 75541023.2727 - acc: 0.0000e+00\n",
      "Epoch 165/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 70492156.3636 - acc: 0.0000e+00\n",
      "Epoch 166/250\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 76850303.2727 - acc: 0.0000e+00\n",
      "Epoch 167/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 71897269.8182 - acc: 0.0000e+00\n",
      "Epoch 168/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 71435129.8182 - acc: 0.0000e+00\n",
      "Epoch 169/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 66630133.0909 - acc: 0.0000e+00\n",
      "Epoch 170/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 71485946.1818 - acc: 0.0000e+00\n",
      "Epoch 171/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 76616597.0909 - acc: 0.0000e+00\n",
      "Epoch 172/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 65027963.6364 - acc: 0.0000e+00\n",
      "Epoch 173/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 70889910.5455 - acc: 0.0000e+00\n",
      "Epoch 174/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 70439480.7273 - acc: 0.0000e+00\n",
      "Epoch 175/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 76020564.3636 - acc: 0.0000e+00\n",
      "Epoch 176/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 74415312.7273 - acc: 0.0000e+00\n",
      "Epoch 177/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 74891941.0909 - acc: 0.0000e+00\n",
      "Epoch 178/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 69831758.1818 - acc: 0.0000e+00\n",
      "Epoch 179/250\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 70941145.8182 - acc: 0.0015\n",
      "Epoch 180/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 70830377.4545 - acc: 0.0000e+00\n",
      "Epoch 181/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 74772711.2727 - acc: 0.0031\n",
      "Epoch 182/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 78342642.9091 - acc: 0.0000e+00\n",
      "Epoch 183/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 70320090.9091 - acc: 0.0000e+00\n",
      "Epoch 184/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 72008159.2727 - acc: 0.0000e+00\n",
      "Epoch 185/250\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 71998221.0909 - acc: 0.0000e+00\n",
      "Epoch 186/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 74966765.0909 - acc: 0.0000e+00\n",
      "Epoch 187/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 70217112.7273 - acc: 0.0000e+00\n",
      "Epoch 188/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 69460248.7273 - acc: 0.0020\n",
      "Epoch 189/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 74128610.9091 - acc: 0.0000e+00\n",
      "Epoch 190/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 71749763.6364 - acc: 0.0000e+00\n",
      "Epoch 191/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 71133833.8182 - acc: 0.0000e+00\n",
      "Epoch 192/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 66301331.2727 - acc: 0.0000e+00\n",
      "Epoch 193/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 70928940.0000 - acc: 0.0000e+00\n",
      "Epoch 194/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 75300012.3636 - acc: 0.0000e+00\n",
      "Epoch 195/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 77228181.0909 - acc: 0.0000e+00\n",
      "Epoch 196/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 70746951.2727 - acc: 0.0000e+00\n",
      "Epoch 197/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 71870120.7273 - acc: 0.0000e+00\n",
      "Epoch 198/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 64882595.2727 - acc: 0.0000e+00\n",
      "Epoch 199/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 71689299.6364 - acc: 0.0000e+00\n",
      "Epoch 200/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 74183177.4545 - acc: 0.0000e+00\n",
      "Epoch 201/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 72992576.7273 - acc: 0.0000e+00\n",
      "Epoch 202/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 66566693.4545 - acc: 0.0000e+00\n",
      "Epoch 203/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 67165322.1818 - acc: 0.0000e+00\n",
      "Epoch 204/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 74395986.1818 - acc: 0.0000e+00\n",
      "Epoch 205/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 74769226.1818 - acc: 0.0000e+00\n",
      "Epoch 206/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 71683726.1818 - acc: 0.0000e+00\n",
      "Epoch 207/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 72209951.2727 - acc: 0.0000e+00\n",
      "Epoch 208/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 69276557.4545 - acc: 0.0019\n",
      "Epoch 209/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 74370424.7273 - acc: 0.0000e+00\n",
      "Epoch 210/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 70524652.7273 - acc: 0.0000e+00\n",
      "Epoch 211/250\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 73819680.0000 - acc: 0.0000e+00\n",
      "Epoch 212/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 73360037.0909 - acc: 0.0050\n",
      "Epoch 213/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 71305322.1818 - acc: 0.0000e+00\n",
      "Epoch 214/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 71782061.8182 - acc: 0.0000e+00\n",
      "Epoch 215/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 75721379.6364 - acc: 0.0000e+00\n",
      "Epoch 216/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 74630668.3636 - acc: 0.0000e+00\n",
      "Epoch 217/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 73847773.0909 - acc: 0.0000e+00\n",
      "Epoch 218/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 75095724.3636 - acc: 0.0000e+00\n",
      "Epoch 219/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 77546913.4545 - acc: 0.0066\n",
      "Epoch 220/250\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 74839186.9091 - acc: 0.0000e+00\n",
      "Epoch 221/250\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 69055157.4545 - acc: 0.0000e+00\n",
      "Epoch 222/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 74467579.6364 - acc: 0.0000e+00\n",
      "Epoch 223/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 69010550.1818 - acc: 0.0000e+00\n",
      "Epoch 224/250\n",
      "10/10 [==============================] - 0s 20ms/step - loss: 75186610.9091 - acc: 0.0000e+00\n",
      "Epoch 225/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 66562327.2727 - acc: 0.0000e+00\n",
      "Epoch 226/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 72798144.0000 - acc: 0.0000e+00\n",
      "Epoch 227/250\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 67754448.0000 - acc: 0.0000e+00\n",
      "Epoch 228/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 70000132.3636 - acc: 0.0000e+00\n",
      "Epoch 229/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 68120375.6364 - acc: 0.0000e+00\n",
      "Epoch 230/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 67271594.9091 - acc: 0.0000e+00\n",
      "Epoch 231/250\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 73617461.8182 - acc: 0.0000e+00\n",
      "Epoch 232/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 64698642.5455 - acc: 0.0024\n",
      "Epoch 233/250\n",
      "10/10 [==============================] - 0s 31ms/step - loss: 67885067.6364 - acc: 0.0000e+00\n",
      "Epoch 234/250\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 67436798.9091 - acc: 0.0000e+00\n",
      "Epoch 235/250\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 74420746.1818 - acc: 0.0000e+00\n",
      "Epoch 236/250\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 73855395.6364 - acc: 6.4935e-04\n",
      "Epoch 237/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 70882733.0909 - acc: 0.0000e+00\n",
      "Epoch 238/250\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 69602069.8182 - acc: 0.0000e+00\n",
      "Epoch 239/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 76883993.4545 - acc: 0.0000e+00\n",
      "Epoch 240/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 70252862.1818 - acc: 0.0000e+00\n",
      "Epoch 241/250\n",
      "10/10 [==============================] - 0s 22ms/step - loss: 66068343.6364 - acc: 0.0000e+00\n",
      "Epoch 242/250\n",
      "10/10 [==============================] - 0s 19ms/step - loss: 73908861.8182 - acc: 0.0000e+00\n",
      "Epoch 243/250\n",
      "10/10 [==============================] - 0s 30ms/step - loss: 73119448.7273 - acc: 0.0000e+00\n",
      "Epoch 244/250\n",
      "10/10 [==============================] - 0s 25ms/step - loss: 74088352.7273 - acc: 0.0000e+00\n",
      "Epoch 245/250\n",
      "10/10 [==============================] - 0s 24ms/step - loss: 71907418.5455 - acc: 0.0000e+00\n",
      "Epoch 246/250\n",
      "10/10 [==============================] - 0s 23ms/step - loss: 73947361.4545 - acc: 0.0031\n",
      "Epoch 247/250\n",
      "10/10 [==============================] - 0s 29ms/step - loss: 78649576.7273 - acc: 0.0000e+00\n",
      "Epoch 248/250\n",
      "10/10 [==============================] - 0s 27ms/step - loss: 71761563.2727 - acc: 0.0000e+00\n",
      "Epoch 249/250\n",
      "10/10 [==============================] - 0s 21ms/step - loss: 68577980.3636 - acc: 0.0000e+00\n",
      "Epoch 250/250\n",
      "10/10 [==============================] - 0s 28ms/step - loss: 69361375.6364 - acc: 0.0066\n"
     ]
    }
   ],
   "source": [
    "history = model.fit({'graph_input': [nodes_tr_davis, adj_tr_davis], 'cnn_input': proteins_tr_davis_reshape }, affinity_tr_davis,batch_size=28, epochs=250,steps_per_epoch=10)"
   ]
  },
  {
   "source": [
    "Build a 2D CNN based on graph DAVIS data\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'num_drugs' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d8251195be1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mnodes_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn2d_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_drugs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0madj_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn2d_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_prot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprot_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_drugs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nodes = nodes_ts_davis[0]\n",
    "adj = adj_ts_davis[0]\n",
    "\n",
    "def cnn1d(input_dim):\n",
    "    cnn = Sequential() # Create sequential model\n",
    "    cnn.add(Conv1D(16, 3, activation='relu', input_shape=(input_dim, 1)))\n",
    "    cnn.add(MaxPooling1D(3))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.1))\n",
    "    cnn.add(Dense(16, activation = 'linear'))\n",
    "    return cnn\n",
    "\n",
    "def cnn2d_nodes(nodes):\n",
    "    cnn = Sequential() # Create sequential model\n",
    "    cnn.add(Conv2D(16, 3, activation='relu', input_shape=nodes.shape))\n",
    "    cnn.add(MaxPooling2D(3))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.1))\n",
    "    cnn.add(Dense(16, activation = 'linear'))\n",
    "    return cnn\n",
    "\n",
    "def cnn2d_adj(adj):\n",
    "    cnn = Sequential() # Create sequential model\n",
    "    cnn.add(Conv2D(16, 3, activation='relu', input_shape=adj.shape))\n",
    "    cnn.add(MaxPooling2D(3))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dropout(0.1))\n",
    "    cnn.add(Dense(16, activation = 'linear'))\n",
    "    return cnn\n",
    "\n",
    "nodes_cnn = cnn2d_nodes(nodes)\n",
    "adj_cnn = cnn2d_adj(adj)\n",
    "prot_cnn = cnn1d(input_dim)\n",
    "cnn_concat = concatenate([nodes_cnn.output, adj_cnn.output, prot_cnn.output])\n",
    "\n",
    "final_concat = Model(inputs=[[nodes_cnn.input, adj_cnn.input], prot_cnn.input], outputs=cnn_concat)\n",
    "\n",
    "\n",
    "final_concat = Dense(1024, activation='relu')(final_concat)\n",
    "final_concat = Dropout(0.1)(cnn_concat)\n",
    "final_concat = Dense(16, activation='relu')(final_concat)\n",
    "\n",
    "final = Dense(1, activation='linear')(final_concat)\n",
    "\n",
    "\n",
    "# Show model summary\n",
    "final.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert SMILES to graphs\n",
    "smiles_kiba, convmol_kiba = smiles_graph(LOCAL_KIBA_PATH)\n",
    "smiles_davis, convmol_davis = smiles_graph(LOCAL_DAVIS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is from mini project\n",
    "# for converting protein sequence to categorical format/numerical format\n",
    "seq_voc = \"ABCDEFGHIKLMNOPQRSTUVWXYZ\"\n",
    "seq_dict = {v:i for i,v in enumerate(seq_voc)}\n",
    "seq_dict_len = len(seq_dict)\n",
    "max_seq_len = 1000   # Note that all protein data will have the same length 1000 \n",
    "\n",
    "def seq_to_cat(prot):  # prot: protein\n",
    "    x = np.zeros(max_seq_len)\n",
    "    for i, ch in enumerate(prot[:max_seq_len]): \n",
    "        x[i] = seq_dict[ch]\n",
    "    return x  \n"
   ]
  },
  {
   "source": [
    "Read in Kiba data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpath = G_PATH + '/data_Drug_target_binding_affinity/data/kiba/'\n",
    "fpath = LOCAL_KIBA_PATH \n",
    "\n",
    "# Read in drugs and proteins\n",
    "drugs_ = json.load(open(fpath + \"ligands_can.txt\"), object_pairs_hook=OrderedDict)\n",
    "drugs = np.array([Chem.MolToSmiles(Chem.MolFromSmiles(d),isomericSmiles=True) for d in drugs_.values()])\n",
    "proteins_ = json.load(open(fpath + \"proteins.txt\"), object_pairs_hook=OrderedDict)\n",
    "proteins = np.array(list(proteins_.values()))\n",
    "\n",
    "# Read in affinity data\n",
    "affinity = np.array(pickle.load(open(fpath + \"Y\",\"rb\"), encoding='latin1'))\n",
    "\n",
    "# Read in train/test fold  \n",
    "train_fold = json.load(open(fpath + \"folds/train_fold_setting1.txt\"))\n",
    "train_fold = [ee for e in train_fold for ee in e ]    \n",
    "'''\n",
    "Here all validation folds are aggregated into training set. \n",
    "If you want to train models with different architectures and/or \n",
    "optimize for model hyperparameters, we encourage you to use 5-fold \n",
    "cross validation as provided here.\n",
    "'''\n",
    "test_fold = json.load(open(fpath + \"folds/test_fold_setting1.txt\"))\n",
    "\n",
    "# Prepare train/test data with fold indices\n",
    "rows, cols = np.where(np.isnan(affinity)==False) \n",
    "drugs_tr = drugs[rows[train_fold]]    # (98545,)\n",
    "proteins_tr = np.array([seq_to_cat(p) for p in proteins[cols[train_fold]]])   # (98545, 1000)\n",
    "affinity_tr = affinity[rows[train_fold], cols[train_fold]]  # (98545,)\n",
    "\n",
    "drugs_ts = drugs[rows[test_fold]] # (19709,)\n",
    "proteins_ts = np.array([seq_to_cat(p) for p in proteins[cols[test_fold]]]) # (19709, 1000)\n",
    "affinity_ts = affinity[rows[test_fold], cols[test_fold]]    # (19709,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to ECFP fingerprint\n",
    "smileToMol = lambda x: MolFromSmiles(x)  # molecules from smiles\n",
    "featurizer = dc.feat.CircularFingerprint(size=1024)\n",
    "\n",
    "drugs_mol_tr = list(map(smileToMol, drugs_tr))\n",
    "drugs_ecfp_tr = featurizer.featurize(drugs_mol_tr)\n",
    "drugs_mol_ts = list(map(smileToMol, drugs_ts))\n",
    "drugs_ecfp_ts = featurizer.featurize(drugs_mol_ts)\n",
    "\n",
    "print(drugs_ecfp_tr.shape)\n",
    "print(drugs_ecfp_ts.shape)\n",
    "\n",
    "# Save to local computer\n",
    "# np.savetxt(LOCAL_PATH + '/saved_data_local/drugs_ecfp_tr.csv', drugs_ecfp_tr, delimiter= ',')\n",
    "# np.savetxt(LOCAL_PATH + '/saved_data_local/drugs_ecfp_ts.csv', drugs_ecfp_ts, delimiter= ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read drugs_edfp from the saved files\n",
    "drugs_ecfp_ts = np.loadtxt('../saved_data_local/drugs_ecfp_ts.csv', delimiter = ',')\n",
    "drugs_ecfp_tr = np.loadtxt('../saved_data_local/drugs_ecfp_tr.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_size, drug_size = drugs_ecfp_tr.shape[0], drugs_ecfp_tr.shape[1]\n",
    "ts_size = drugs_ecfp_ts.shape[0]\n",
    "\n",
    "protein_size = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train, num_drugs = drugs_ecfp_tr.shape\n",
    "num_prot = proteins_tr.shape[1]\n",
    "drugs_tr_reshape = drugs_ecfp_tr.reshape((num_train, num_drugs, 1))\n",
    "proteins_tr_reshape = proteins_tr.reshape((num_train, num_prot, 1))\n",
    "\n",
    "# Testing data\n",
    "drug_ts_reshape = drugs_ecfp_ts.reshape((drugs_ecfp_ts.shape[0], drugs_ecfp_ts.shape[1], 1))\n",
    "proteins_ts_reshape = proteins_ts.reshape((proteins_ts.shape[0], proteins_ts.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(98545, 1024)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "drugs_ecfp_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_26 (InputLayer)           [(None, 1024, 1)]    0                                            \n__________________________________________________________________________________________________\ninput_27 (InputLayer)           [(None, 1024, 1)]    0                                            \n__________________________________________________________________________________________________\ngraph_input (ChebConv)          (None, 1024, 64)     128         input_26[0][0]                   \n                                                                 input_27[0][0]                   \n__________________________________________________________________________________________________\ndropout_23 (Dropout)            (None, 1024, 64)     0           graph_input[0][0]                \n__________________________________________________________________________________________________\ninput_25 (InputLayer)           [(None, 1000, 1)]    0                                            \n__________________________________________________________________________________________________\ncheb_conv_10 (ChebConv)         (None, 1024, 32)     2080        dropout_23[0][0]                 \n                                                                 input_27[0][0]                   \n__________________________________________________________________________________________________\nconv1d_9 (Conv1D)               (None, 998, 16)      64          input_25[0][0]                   \n__________________________________________________________________________________________________\ndropout_24 (Dropout)            (None, 1024, 32)     0           cheb_conv_10[0][0]               \n__________________________________________________________________________________________________\nmax_pooling1d_9 (MaxPooling1D)  (None, 332, 16)      0           conv1d_9[0][0]                   \n__________________________________________________________________________________________________\ncheb_conv_11 (ChebConv)         (None, 1024, 32)     1056        dropout_24[0][0]                 \n                                                                 input_27[0][0]                   \n__________________________________________________________________________________________________\nflatten_13 (Flatten)            (None, 5312)         0           max_pooling1d_9[0][0]            \n__________________________________________________________________________________________________\nflatten_14 (Flatten)            (None, 32768)        0           cheb_conv_11[0][0]               \n__________________________________________________________________________________________________\ndropout_22 (Dropout)            (None, 5312)         0           flatten_13[0][0]                 \n__________________________________________________________________________________________________\ndense_24 (Dense)                (None, 16)           524304      flatten_14[0][0]                 \n__________________________________________________________________________________________________\ndense_23 (Dense)                (None, 16)           85008       dropout_22[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 32)           0           dense_24[0][0]                   \n                                                                 dense_23[0][0]                   \n__________________________________________________________________________________________________\ndense_25 (Dense)                (None, 512)          16896       concatenate_5[0][0]              \n__________________________________________________________________________________________________\ndense_26 (Dense)                (None, 16)           8208        dense_25[0][0]                   \n__________________________________________________________________________________________________\ndense_27 (Dense)                (None, 1)            17          dense_26[0][0]                   \n==================================================================================================\nTotal params: 637,761\nTrainable params: 637,761\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN for protein\n",
    "cnn_input = Input(shape=(num_prot,1))\n",
    "cnn_layer = Conv1D(16, 3, activation='relu', input_shape=(num_prot, 1))(cnn_input)\n",
    "cnn_layer = MaxPooling1D(3)(cnn_layer)\n",
    "cnn_layer = Flatten()(cnn_layer)\n",
    "cnn_layer = Dropout(0.1)(cnn_layer)\n",
    "cnn_layer = Dense(16, activation = 'linear')(cnn_layer)\n",
    "\n",
    "# xs1, xs2 = nodes_ts_davis.shape[1], nodes_ts_davis.shape[2]\n",
    "# as1, as2 = adj_tr_davis.shape[1], adj_tr_davis.shape[2]\n",
    "X_in = Input(shape=(num_drugs, 1))\n",
    "A_in = Input(shape = (num_drugs, 1))\n",
    "\n",
    "# GNN for drugs\n",
    "graph_conv = ChebConv(64, activation='relu',name='graph_input', kernel_regularizer='l1')([X_in, A_in])\n",
    "graph_conv = Dropout(0.1)(graph_conv)\n",
    "\n",
    "graph_conv = ChebConv(32, activation='relu', kernel_regularizer='l1')([graph_conv,A_in])\n",
    "\n",
    "graph_conv = Dropout(0.1)(graph_conv)\n",
    "\n",
    "# graph_conv = GATConv(64, activation='relu')([graph_conv,A_in])\n",
    "# graph_conv = Dropout(0.5)(graph_conv)\n",
    "graph_conv = ChebConv(32, activation='relu')([graph_conv, A_in])\n",
    "\n",
    "graph_conv = Flatten()(graph_conv)\n",
    "graph_conv = Dense(16, activation= 'linear')(graph_conv)\n",
    "\n",
    "dta_concat = concatenate([graph_conv, cnn_layer])\n",
    "\n",
    "dta_concat = Dense(512, activation='relu')(dta_concat)\n",
    "dta_concat = Dense(16, activation='relu')(dta_concat)\n",
    "dta_concat = Dense(1, activation='softmax')(dta_concat)\n",
    "\n",
    "model = Model(inputs={'graph_input':[X_in, A_in], 'cnn_input':cnn_input}, outputs=dta_concat)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 2s 63ms/step - loss: 117.0641 - mean_squared_error: 114.4603\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 116.1204 - mean_squared_error: 113.7198\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 116.6083 - mean_squared_error: 114.4017\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 116.2542 - mean_squared_error: 114.2350\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 117.3981 - mean_squared_error: 115.5598\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 115.9933 - mean_squared_error: 114.3276\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 118.5910 - mean_squared_error: 117.0901\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 117.5895 - mean_squared_error: 116.2445\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 116.0487 - mean_squared_error: 114.8504\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 1s 52ms/step - loss: 116.2095 - mean_squared_error: 115.1494\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 115.7437 - mean_squared_error: 114.8126\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 113.4964 - mean_squared_error: 112.6863\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 115.3367 - mean_squared_error: 114.6389\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 116.8945 - mean_squared_error: 116.2999\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 115.9869 - mean_squared_error: 115.4870\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 116.3624 - mean_squared_error: 115.9484\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 116.6216 - mean_squared_error: 116.2848\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 114.6119 - mean_squared_error: 114.3429\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 117.3002 - mean_squared_error: 117.0920\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 115.1285 - mean_squared_error: 114.9733\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 114.9919 - mean_squared_error: 114.8814\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 117.8365 - mean_squared_error: 117.7623\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 115.5772 - mean_squared_error: 115.5320\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 117.4267 - mean_squared_error: 117.4023\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 116.3826 - mean_squared_error: 116.3709\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 115.1921 - mean_squared_error: 115.1844\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 115.3191 - mean_squared_error: 115.3141\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 114.3359 - mean_squared_error: 114.3319\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 1s 55ms/step - loss: 118.8426 - mean_squared_error: 118.8393\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 48ms/step - loss: 116.4687 - mean_squared_error: 116.4658\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 115.9587 - mean_squared_error: 115.9558\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 111.8830 - mean_squared_error: 111.8803\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 115.6196 - mean_squared_error: 115.6170\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 117.2331 - mean_squared_error: 117.2305\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 46ms/step - loss: 118.4448 - mean_squared_error: 118.4421\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 115.7213 - mean_squared_error: 115.7186\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 114.9850 - mean_squared_error: 114.9824\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 116.0420 - mean_squared_error: 116.0393\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 118.9816 - mean_squared_error: 118.9789\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 113.5862 - mean_squared_error: 113.5835\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 112.5856 - mean_squared_error: 112.5829\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 113.0206 - mean_squared_error: 113.0179\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 116.8881 - mean_squared_error: 116.8854\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 43ms/step - loss: 112.7681 - mean_squared_error: 112.7654\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 116.9753 - mean_squared_error: 116.9726\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 118.4723 - mean_squared_error: 118.4696\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 1s 51ms/step - loss: 114.7142 - mean_squared_error: 114.7115\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 45ms/step - loss: 118.6926 - mean_squared_error: 118.6899\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 49ms/step - loss: 113.1376 - mean_squared_error: 113.1349\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 118.3956 - mean_squared_error: 118.3929\n"
     ]
    }
   ],
   "source": [
    "history = model.fit({'graph_input': [drugs_tr_reshape, drugs_tr_reshape], 'cnn_input': proteins_tr_reshape }, affinity_tr,batch_size=28, epochs=50,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluate on test data\n",
      "154/154 [==============================] - 4s 22ms/step - loss: 115.5841 - mean_squared_error: 115.5814\n",
      "test MSE loss is: [115.58407592773438, 115.5813980102539]\n"
     ]
    }
   ],
   "source": [
    "drug_ts_reshape = drugs_ecfp_ts.reshape((drugs_ecfp_ts.shape[0], drugs_ecfp_ts.shape[1], 1))\n",
    "proteins_ts_reshape = proteins_ts.reshape((proteins_ts.shape[0], proteins_ts.shape[1], 1))\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate({'graph_input': [drug_ts_reshape, drug_ts_reshape], 'cnn_input': proteins_ts_reshape }, affinity_ts, batch_size=128)\n",
    "print(\"test MSE loss is:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}